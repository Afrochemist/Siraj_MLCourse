{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common packages\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras packages\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plant the seeds\n",
    "seed = 123\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters used for the model\n",
    "discount_factor = 0.99\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 32\n",
    "train_start = 1000\n",
    "memory_size = 10000\n",
    "n_episodes = 2000\n",
    "n_win_ticks = 195\n",
    "n_avg_scores = 100\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(96, input_dim=state_size, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(48, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(24, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(action_size, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.compile(Adam(lr=0.001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 96)                480       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 48)                4656      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 6,362\n",
      "Trainable params: 6,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "target_model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "# Source: https://github.com/yanpanlau/CartPole/blob/master/DQN/CartPole_DQN.py\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def get_action(state, epsilon):\n",
    "    return np.random.randint(action_size) if np.random.rand() <= epsilon else np.argmax(model.predict(state)[0])\n",
    "\n",
    "def train_replay():\n",
    "    if len(memory) < train_start:\n",
    "        return\n",
    "    minibatch = random.sample(memory,  min(batch_size, len(memory)))\n",
    "    state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
    "    state_t = np.concatenate(state_t)\n",
    "    state_t1 = np.concatenate(state_t1)\n",
    "    targets = model.predict(state_t)\n",
    "    Q_sa = target_model.predict(state_t1)\n",
    "    targets[range(batch_size), action_t] = reward_t + discount_factor * np.max(Q_sa, axis=1) * np.invert(terminal)\n",
    "    model.train_on_batch(state_t, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "avg_scores = []\n",
    "all_scores = []\n",
    "scores = deque(maxlen=n_avg_scores)\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "def learn_to_balance():\n",
    "    epsilon = 1.0 # Start with randomness\n",
    "    has_won = False\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            action = get_action(state, epsilon)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            memory.append((state, action, reward if not done else -100, next_state, done))\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay # Decrease randomness\n",
    "            train_replay()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                env.reset()\n",
    "                update_target_model()\n",
    "                scores.append(score)\n",
    "                all_scores.append(score)\n",
    "                avg_score = np.mean(scores)\n",
    "                avg_scores.append(avg_score)\n",
    "\n",
    "                if not has_won and e >= n_avg_scores and avg_score >= n_win_ticks:\n",
    "                    # Find first score greater than 195 where the average is >= 195 over the following 100 episodes.\n",
    "                    solution_episode_idx = max(next(x[0] for x in enumerate(all_scores) if x[1] >= n_win_ticks), e - n_win_ticks) \n",
    "                    print('Solved after {} tries! \\o/'.format(solution_episode_idx))\n",
    "                    has_won = True\n",
    "\n",
    "                if e % n_avg_scores == 0:\n",
    "                    print('[Episode {}] Average Score: {}'.format(e, avg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] Average Score: 14.0\n",
      "WARNING:tensorflow:From C:\\Users\\abrooks\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[Episode 100] Average Score: 168.31\n",
      "Solved after 42 tries! \\o/\n",
      "[Episode 200] Average Score: 322.44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "# The challenge is solved at episode 8. The mean score remains >= 195 for the following 100 episodes.\n",
    "# A higher score is very likely achievable through hyperparameter optimization and seed exploration.\n",
    "# A better performance metric would span more episodes and cover a range of initial environment seeds.\n",
    "learn_to_balance()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(all_scores, color='blue')\n",
    "plt.plot(avg_scores, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
